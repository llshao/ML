{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "H = 128 # Number of LSTM layer's neurons \n",
    "D = 128 # Number of input dimension == number of items in vocabulary \n",
    "Z = H + D # Because we will concatenate LSTM state with the input \n",
    "model = dict(\n",
    "    Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "    Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "    Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "    Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "    Wy=np.random.randn(H, D) / np.sqrt(D / 2.),\n",
    "    bf=np.zeros((1, H)),\n",
    "    bi=np.zeros((1, H)),\n",
    "    bc=np.zeros((1, H)),\n",
    "    bo=np.zeros((1, H)),\n",
    "    by=np.zeros((1, D))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(X, state):\n",
    "    m = model\n",
    "    Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "    bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "    h_old, c_old = state\n",
    "\n",
    "    # One-hot encode     \n",
    "    X_one_hot = np.zeros(D)\n",
    "    X_one_hot[X] = 1.\n",
    "    X_one_hot = X_one_hot.reshape(1, -1)\n",
    "\n",
    "    # Concatenate old state with current input     \n",
    "    X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "    hf = sigmoid(X @ Wf + bf)\n",
    "    hi = sigmoid(X @ Wi + bi)\n",
    "    ho = sigmoid(X @ Wo + bo)\n",
    "    hc = tanh(X @ Wc + bc)\n",
    "\n",
    "    c = hf * c_old + hi * hc\n",
    "    h = ho * tanh(c)\n",
    "\n",
    "    y = h @ Wy + by\n",
    "    prob = softmax(y)\n",
    "\n",
    "    state = (h, c) # Cache the states of current h & c for next iter     cache = ... # Add all intermediate variables to this cache \n",
    "    return prob, state, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(prob, y_train, d_next, cache):\n",
    "    # Unpack the cache variable to get the intermediate variables used in forward step     ... = cache\n",
    "    dh_next, dc_next = d_next\n",
    "\n",
    "    # Softmax loss gradient     dy = prob.copy()\n",
    "    dy[1, y_train] -= 1.\n",
    "\n",
    "    # Hidden to output gradient     dWy = h.T @ dy\n",
    "    dby = dy\n",
    "    # Note we're adding dh_next here     dh = dy @ Wy.T + dh_next\n",
    "\n",
    "    # Gradient for ho in h = ho * tanh(c)     dho = tanh(c) * dh\n",
    "    dho = dsigmoid(ho) * dho\n",
    "\n",
    "    # Gradient for c in h = ho * tanh(c), note we're adding dc_next here     dc = ho * dh * dtanh(c)\n",
    "    dc = dc + dc_next\n",
    "\n",
    "    # Gradient for hf in c = hf * c_old + hi * hc     dhf = c_old * dc\n",
    "    dhf = dsigmoid(hf) * dhf\n",
    "\n",
    "    # Gradient for hi in c = hf * c_old + hi * hc     dhi = hc * dc\n",
    "    dhi = dsigmoid(hi) * dhi\n",
    "\n",
    "    # Gradient for hc in c = hf * c_old + hi * hc     dhc = hi * dc\n",
    "    dhc = dtanh(hc) * dhc\n",
    "\n",
    "    # Gate gradients, just a normal fully connected layer gradient     dWf = X.T @ dhf\n",
    "    dbf = dhf\n",
    "    dXf = dhf @ Wf.T\n",
    "\n",
    "    dWi = X.T @ dhi\n",
    "    dbi = dhi\n",
    "    dXi = dhi @ Wi.T\n",
    "\n",
    "    dWo = X.T @ dho\n",
    "    dbo = dho\n",
    "    dXo = dho @ Wo.T\n",
    "\n",
    "    dWc = X.T @ dhc\n",
    "    dbc = dhc\n",
    "    dXc = dhc @ Wc.T\n",
    "\n",
    "    # As X was used in multiple gates, the gradient must be accumulated here     dX = dXo + dXc + dXi + dXf\n",
    "    # Split the concatenated X, so that we get our gradient of h_old     dh_next = dX[:, :H]\n",
    "    # Gradient for c_old in c = hf * c_old + hi * hc     dc_next = hf * dc\n",
    "\n",
    "    grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "    state = (dh_next, dc_next)\n",
    "\n",
    "    return grad, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X_train, y_train, state):\n",
    "    probs = []\n",
    "    caches = []\n",
    "    loss = 0.\n",
    "    h, c = state\n",
    "\n",
    "    # Forward Step \n",
    "    for x, y_true in zip(X_train, y_train):\n",
    "        prob, state, cache = lstm_forward(x, state, train=True)\n",
    "        loss += cross_entropy(prob, y_true)\n",
    "\n",
    "        # Store forward step result to be used in backward step         probs.append(prob)\n",
    "        caches.append(cache)\n",
    "\n",
    "    # The loss is the average cross entropy     loss /= X_train.shape[0]\n",
    "\n",
    "    # Backward Step \n",
    "    # Gradient for dh_next and dc_next is zero for the last timestep     d_next = (np.zeros_like(h), np.zeros_like(c))\n",
    "    grads = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "\n",
    "    # Go backward from the last timestep to the first     for prob, y_true, cache in reversed(list(zip(probs, y_train, caches))):\n",
    "    grad, d_next = lstm_backward(prob, y_true, d_next, cache)\n",
    "\n",
    "    # Accumulate gradients from all timesteps         for k in grads.keys():\n",
    "    grads[k] += grad[k]\n",
    "\n",
    "    return grads, loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
